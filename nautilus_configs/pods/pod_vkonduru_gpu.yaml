# Please look at job template for more comments and explanations
apiVersion: v1
kind: Pod
metadata:
  # team${TEAM_ID}-${USER_NAME}-{EXP_NAME}
  name: team1-ani-explore
spec:
  containers:
  - name: ani-gpu-container
    # image from gitlab-registry.nautilus.optiputer.net, or some official ones provided
    image: trn84/repo:latest
    # image: pytorch/pytorch:1.3-cuda9.1-cudnn7-devel
    args: ["sleep", "infinity"]
    resources:
      requests:
        cpu: "2"
        memory: "12Gi"
        nvidia.com/gpu: 1
      limits:
        cpu: "2"
        memory: "12Gi"
        nvidia.com/gpu: 1
    volumeMounts:
    - name: dshm
      mountPath: /dev/shm
    - name: cephfs-shared
      mountPath: /shared
    - name: cephfs-team1
      mountPath: /team1
    - name: cephfs-waymo-od
      mountPath: /waymo-od
  # restartPolicy: Never
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
    - name: cephfs-shared
      persistentVolumeClaim:
        claimName: cephfs-shared
    - name: cephfs-team1
      persistentVolumeClaim:
        claimName: cephfs-team1
    - name: cephfs-waymo-od
      persistentVolumeClaim:
        claimName: cephfs-waymo-od        
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
        #   - key: nautilus.io/group
        #     operator: In
        #     values:
        #     - haosu
          - key: gpu-type
            operator: In # Use NotIn for other types
            values:
            - 2080Ti
            - 1080Ti

